{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd278c8b-2f9e-41e0-a1f1-b39ca78d0117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/hrf78r2d3pn5xzgwhhlhb0fm0000gn/T/ipykernel_30844/295845660.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import albumentations\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tokenizers\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8fa141-fe44-4ee8-bffb-9481804740f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolos, Conditional DETR & DETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad852ca1-dbf4-4841-9a2c-19075b849b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"training_dataset\"\n",
    "TRAIN_DATA = \"train_data.json\"\n",
    "VAL_DATA = \"val_data.json\"\n",
    "IMAGE_SIZE = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebdbd260-8a78-4100-90a5-1e472eb35d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"Download the training & validation datatsets\"\"\"\n",
    "\n",
    "    dataset_artifact = wandb.use_artifact(f'{params.FINAL_DATA_AT}:latest')\n",
    "    dataset_path = Path(dataset_artifact.download())\n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bfa3c80-de85-44cc-9193-5638f79d9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: dict, data_path: str):\n",
    "    \"\"\"Convert data format\n",
    "\n",
    "    Convert data format from json with multiple keys to a list of [image_info, image annotations]\n",
    "    :param data: input data\n",
    "    :type data: json\n",
    "    :param data_path: path to images\n",
    "    :type data_path: str\n",
    "    :return: converted data into a list\n",
    "    :rtype: [dict, [dict]]\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for image_data in tqdm(data[\"images\"]):\n",
    "        image_path = os.path.join(data_path, image_data[\"file_name\"])\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "        \n",
    "        height = image_data[\"height\"]\n",
    "        width = image_data[\"width\"]\n",
    "        curr_image_id = image_data[\"id\"]\n",
    "        curr_annos = []\n",
    "        for anno in data[\"annotations\"]:\n",
    "            if anno[\"image_id\"] != curr_image_id:\n",
    "                continue\n",
    "            \n",
    "            x, y, w, h = anno[\"bbox\"]\n",
    "            if w == 0 or h == 0:\n",
    "                continue\n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            if y < 0:\n",
    "                y = 0\n",
    "    \n",
    "            if x >= width:\n",
    "                continue\n",
    "            if x + w > width:\n",
    "                w = width - x\n",
    "            \n",
    "            if y >= height:\n",
    "                continue\n",
    "            if y + h > height:\n",
    "                y = height - y\n",
    "            \n",
    "            anno[\"bbox\"] = [x, y, w, h]\n",
    "            curr_annos.append(anno)\n",
    "\n",
    "        if len(curr_annos) == 0:\n",
    "            continue\n",
    "    \n",
    "        curr_image_data = image_data.copy()\n",
    "        curr_image_data[\"image_path\"] = image_path\n",
    "        processed_data.append((curr_image_data, curr_annos))\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34d511e2-9383-4ce6-8948-85552cdaf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaWorldDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Tuple[Dict, List[Dict]]], image_processor, is_augment: str = True):\n",
    "        self._data = data\n",
    "        self._image_processor = image_processor\n",
    "\n",
    "        if is_augment:\n",
    "            self._transform = albumentations.Compose(\n",
    "                [\n",
    "                    albumentations.Resize(580, 580),\n",
    "                    albumentations.HorizontalFlip(p=.5),\n",
    "                    albumentations.RandomBrightnessContrast(p=.5),\n",
    "                    albumentations.Rotate(limit=30),\n",
    "                    albumentations.RandomCrop(IMAGE_SIZE, IMAGE_SIZE)\n",
    "                ],\n",
    "                bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"categories\"]),\n",
    "            )\n",
    "        else:\n",
    "            self._transform = albumentations.Compose(\n",
    "                [\n",
    "                    albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                ],\n",
    "                bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"categories\"]),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_processed_annotations(bboxes, categories, image_id):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        processed_annotations = []\n",
    "        for bbox, category_id in zip(bboxes, categories):\n",
    "            processed_annotations.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": category_id,\n",
    "                    \"bbox\": [int(arg + 0.5) for arg in bbox],\n",
    "                    \"isCrowd\": 0,\n",
    "                    \"area\": int(bbox[2] * bbox[3] + 0.5)\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return processed_annotations\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[np.ndarray, List[Dict], int]:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        image_data, annotations = self._data[index]\n",
    "        image_id = image_data[\"id\"]\n",
    "        bboxes, categories = [], []\n",
    "        for annotation in annotations:\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            category_id = annotation[\"category_id\"]\n",
    "            bboxes.append(bbox)\n",
    "            categories.append(category_id)\n",
    "\n",
    "        image = np.array(Image.open(image_data[\"image_path\"]).convert(\"RGB\"))\n",
    "        out = self._transform(\n",
    "            image=image,\n",
    "            bboxes=bboxes,\n",
    "            categories=categories\n",
    "        )\n",
    "        annotations = self._format_processed_annotations(\n",
    "            out[\"bboxes\"], out[\"categories\"], image_id\n",
    "        )\n",
    "\n",
    "        processed_image = self._image_processor.preprocess(\n",
    "            images=image, annotations={\"image_id\": image_id, \"annotations\": annotations}, return_tensors=\"pt\"\n",
    "        )\n",
    "        for key, val in processed_image.items():\n",
    "            processed_image[key] = val[0]\n",
    "\n",
    "        return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b981f4-8238-450a-b84f-62a8a801fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, is_yolo=False):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    if not is_yolo:\n",
    "        batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1877c0-2bc1-451a-b574-83e87e29a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_params(model_name, label2id, id2label, training_mode=\"bbox_classifier\"):\n",
    "    def create_model(checkpoint):\n",
    "        return AutoModelForObjectDetection.from_pretrained(\n",
    "            checkpoint,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    \n",
    "    is_yolo = False\n",
    "    if model_name == \"yolos\":\n",
    "        checkpoint = \"hustvl/yolos-small\"\n",
    "        is_yolo = True\n",
    "        model = create_model(checkpoint)\n",
    "        if training_mode == \"bbox_classifier\":\n",
    "            for param in model.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        if model_name == \"DETA\":\n",
    "            checkpoint = \"jozhang97/deta-swin-large\"\n",
    "        elif model_name == \"CondDETR\":\n",
    "            checkpoint = \"microsoft/conditional-detr-resnet-50\"\n",
    "\n",
    "        model = create_model(checkpoint)\n",
    "        if training_mode == \"bbox_classifier\":\n",
    "            for param in model.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    return model, image_processor, is_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421078c3-0d2c-4ab9-a5f6-481a6f104ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DETA\"\n",
    "training_mode = \"bbox_classifier\" # \"full\"\n",
    "run_name = f\"{model_name}_{training_mode}_tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10029111-4fb9-412a-8832-1565cba9ba54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bdy4rkbj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-breeze-22</strong> at: <a href='https://wandb.ai/matt-zak/fanthom_challenge/runs/bdy4rkbj' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge/runs/bdy4rkbj</a><br/> View job at <a href='https://wandb.ai/matt-zak/fanthom_challenge/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDU3NDI5Mg==/version_details/v0' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDU3NDI5Mg==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240128_161017-bdy4rkbj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bdy4rkbj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/matthewzak/projects/fanthomnet/wandb/run-20240128_161148-3aums7cd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matt-zak/fanthom_challenge/runs/3aums7cd' target=\"_blank\">drawn-grass-23</a></strong> to <a href='https://wandb.ai/matt-zak/fanthom_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matt-zak/fanthom_challenge' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matt-zak/fanthom_challenge/runs/3aums7cd' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge/runs/3aums7cd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact fanthom_final_data:latest, 31335.85MB. 16698 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   16698 of 16698 files downloaded.  \n",
      "Done. 0:0:43.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-grass-23</strong> at: <a href='https://wandb.ai/matt-zak/fanthom_challenge/runs/3aums7cd' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge/runs/3aums7cd</a><br/> View job at <a href='https://wandb.ai/matt-zak/fanthom_challenge/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDU3NDI5Mg==/version_details/v0' target=\"_blank\">https://wandb.ai/matt-zak/fanthom_challenge/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDU3NDI5Mg==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240128_161148-3aums7cd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=params.WANDB_PROJECT, entity=params.ENTITY)\n",
    "data_path = download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db733442-8346-461e-a4b3-38e74b6e556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1190/1190 [00:00<00:00, 7793.00it/s]\n",
      "100%|█████████████████████████████████████| 4760/4760 [00:02<00:00, 1680.45it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, DATASET_FOLDER, TRAIN_DATA), \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(os.path.join(data_path, DATASET_FOLDER, VAL_DATA), \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "id2label = {cat[\"id\"]: cat[\"name\"] for cat in val_data[\"categories\"]}\n",
    "label2id = {name: cat_id for cat_id, name in id2label.items()}\n",
    "val_data = process_data(val_data, \"./data/images/\")\n",
    "train_data = process_data(train_data, \"./data/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d85a35d-e871-4022-844d-6a7e47a06fb0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewzak/miniconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/matthewzak/miniconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/matthewzak/miniconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <1FAC2D7E-618C-3A6D-BC20-D8F31AE257E9> /Users/matthewzak/miniconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Some weights of DetaForObjectDetection were not initialized from the model checkpoint at jozhang97/deta-swin-large and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.6.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([133]) in the model instantiated\n",
      "- model.decoder.class_embed.6.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([133, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, image_processor, is_yolo = get_training_params(model_name, label2id, id2label, training_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92cd55fe-2e46-4e82-b799-f1aa5a51ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SeaWorldDataset(train_data, image_processor, is_augment=True)\n",
    "val_dataset = SeaWorldDataset(val_data, image_processor, is_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1be9d3e4-7484-4e34-ab90-f88ac8cc6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=run_name,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd28d3d-968c-40d8-bf38-9eed42e8c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c94ed-8cd9-44ea-a88f-4abccdf392bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
